# app/services/data_service.py
# è³‡æ–™è¼‰å…¥èˆ‡è™•ç†æœå‹™

import logging
import pandas as pd
import os
from datetime import datetime
from app.models.database import db, ComponentRequirement, Material, User, PurchaseOrder, PartDrawingMapping, DeliverySchedule
from sqlalchemy.orm import joinedload

from app.config import FilePaths
from app.utils.helpers import replace_nan_in_dict, get_taiwan_time

app_logger = logging.getLogger(__name__)

class DataService:
    """è³‡æ–™è¼‰å…¥èˆ‡è™•ç†æœå‹™"""
    
    @staticmethod
    def load_and_process_data():
        """
        è¼‰å…¥ä¸¦è™•ç†æ‰€æœ‰è³‡æ–™
        
        Returns:
            åŒ…å«æ‰€æœ‰è™•ç†å¾Œè³‡æ–™çš„å­—å…¸ï¼Œå¤±æ•—å‰‡è¿”å› None
        """
        app_logger.info("é–‹å§‹è¼‰å…¥èˆ‡è™•ç†è³‡æ–™...")
        try:
            # è¼‰å…¥å„å€‹ Excel æª”æ¡ˆ
            df_inventory = pd.read_excel(FilePaths.INVENTORY_FILE)
            df_wip_parts = pd.read_excel(FilePaths.WIP_PARTS_FILE)
            df_finished_parts = pd.read_excel(FilePaths.FINISHED_PARTS_FILE)
            df_prep_semi_finished = pd.read_excel(FilePaths.PREP_SEMI_FINISHED_FILE)
            
            # æ ¹æ“šè¨‚å–®è™Ÿç¢¼é¦–ä½æ•¸å­—ç¯©é¸
            df_wip_parts['è¨‚å–®'] = df_wip_parts['è¨‚å–®'].astype(str)
            df_wip_parts = df_wip_parts[df_wip_parts['è¨‚å–®'].str.startswith(('2', '6'))]
            
            df_finished_parts['è¨‚å–®'] = df_finished_parts['è¨‚å–®'].astype(str)
            df_finished_parts = df_finished_parts[df_finished_parts['è¨‚å–®'].str.startswith(('1', '6'))]
            
            df_prep_semi_finished['è¨‚å–®'] = df_prep_semi_finished['è¨‚å–®'].astype(str)
            df_prep_semi_finished = df_prep_semi_finished[df_prep_semi_finished['è¨‚å–®'].str.startswith(('1', '2', '6'))]
            
            # --- æ–°å¢é‚è¼¯ï¼šè®€å–è³‡æ–™åº«è³‡è¨Š ---
            # 1. è®€å–çµ„ä»¶éœ€æ±‚æ˜ç´°çš„ base_material_id æ¸…å–®
            valid_base_ids = set()
            try:
                valid_base_ids = {r.base_material_id for r in ComponentRequirement.query.all() if r.base_material_id}
                app_logger.info(f"å·²è¼‰å…¥ {len(valid_base_ids)} ç­†çµ„ä»¶éœ€æ±‚æ˜ç´° (base_material_id)")
            except Exception as e:
                app_logger.error(f"è®€å–çµ„ä»¶éœ€æ±‚æ˜ç´°å¤±æ•—: {e}")

            # 2. è®€å–æ¡è³¼äººå“¡åç¨±å°æ‡‰
            buyer_id_to_name_map = {}
            try:
                buyers = User.query.filter_by(role='buyer').all()
                buyer_id_to_name_map = {buyer.id: buyer.full_name for buyer in buyers}
                app_logger.info(f"å·²è¼‰å…¥ {len(buyer_id_to_name_map)} ç­†æ¡è³¼äººå“¡è³‡æ–™")
            except Exception as e:
                app_logger.error(f"è®€å–æ¡è³¼äººå“¡è³‡æ–™å¤±æ•—: {e}")

            # 3. è®€å–ç‰©æ–™èˆ‡æ¡è³¼äººå“¡å°æ‡‰ï¼ˆä½¿ç”¨å‰10ç¢¼ï¼‰
            material_buyer_map = {}
            try:
                materials = Material.query.filter(Material.buyer_id.isnot(None)).all()
                for m in materials:
                    if m.buyer_id and m.base_material_id:
                        # ä½¿ç”¨å‰10ç¢¼ä½œç‚º keyï¼Œå€¼ç‚ºæ¡è³¼äººå“¡å§“å
                        buyer_name = buyer_id_to_name_map.get(m.buyer_id)
                        if buyer_name:
                            material_buyer_map[m.base_material_id] = buyer_name
                        else:
                             # å¦‚æœåœ¨ User è¡¨ä¸­æ‰¾ä¸åˆ°å°æ‡‰çš„æ¡è³¼äººå“¡ï¼Œç›´æ¥ä½¿ç”¨è³‡æ–™åº«ä¸­çš„ ID
                             material_buyer_map[m.base_material_id] = m.buyer_id
                app_logger.info(f"å·²è¼‰å…¥ {len(material_buyer_map)} ç­†ç‰©æ–™æ¡è³¼äººå“¡å°æ‡‰ (ä½¿ç”¨å‰10ç¢¼)")
            except Exception as e:
                app_logger.error(f"è®€å–ç‰©æ–™æ¡è³¼äººå“¡å°æ‡‰å¤±æ•—: {e}")
            
            # 4. è®€å–å“è™Ÿ-åœ–è™Ÿå°ç…§è¡¨
            part_drawing_map = {}
            try:
                mappings = PartDrawingMapping.query.all()
                part_drawing_map = {m.part_number: m.drawing_number for m in mappings}
                app_logger.info(f"å·²è¼‰å…¥ {len(part_drawing_map)} ç­†å“è™Ÿ-åœ–è™Ÿå°ç…§è³‡æ–™")
            except Exception as e:
                app_logger.error(f"è®€å–å“è™Ÿ-åœ–è™Ÿå°ç…§å¤±æ•—: {e}")

            # 5. è®€å–åˆ†æ‰¹äº¤æœŸæ’ç¨‹
            delivery_schedules_map = {}
            try:
                # åªè®€å–æœªå®Œæˆä¸”æœªå–æ¶ˆçš„äº¤æœŸ
                schedules = DeliverySchedule.query.filter(
                    DeliverySchedule.status.notin_(['completed', 'cancelled'])
                ).all()
                for s in schedules:
                    if s.material_id not in delivery_schedules_map:
                        delivery_schedules_map[s.material_id] = []
                    delivery_schedules_map[s.material_id].append({
                        'expected_date': s.expected_date,
                        'quantity': float(s.quantity - (s.received_quantity or 0)),
                        'status': s.status
                    })
                app_logger.info(f"å·²è¼‰å…¥ {len(schedules)} ç­†åˆ†æ‰¹äº¤æœŸæ’ç¨‹è³‡æ–™")
            except Exception as e:
                app_logger.error(f"è®€å–åˆ†æ‰¹äº¤æœŸå¤±æ•—: {e}")
            
            # --- æ–°å¢é‚è¼¯ï¼šæˆå“æ’¥æ–™åˆ†æµ ---
            # 1. å–å¾—æ’¥æ–™.XLSX çš„æ‰€æœ‰ç‰©æ–™å‰10ç¢¼
            wip_base_ids = set(df_wip_parts['ç‰©æ–™'].astype(str).str[:10])
            app_logger.info(f"æ’¥æ–™.XLSX åŒ…å« {len(wip_base_ids)} å€‹ä¸åŒçš„å‰10ç¢¼")
            
            # 2. åˆä½µæ’¥æ–™å‰10ç¢¼å’Œçµ„ä»¶éœ€æ±‚å‰10ç¢¼
            valid_base_ids = valid_base_ids | wip_base_ids
            app_logger.info(f"åˆä½µå¾Œçš„æœ‰æ•ˆå‰10ç¢¼: {len(valid_base_ids)} å€‹")
            
            # 3. è¨ˆç®—æˆå“æ’¥æ–™çš„ base_material_id
            df_finished_parts['base_material_id'] = df_finished_parts['ç‰©æ–™'].astype(str).str[:10]
            
            # 4. åˆ†æµï¼šç¬¦åˆæ¢ä»¶çš„ vs ä¸ç¬¦åˆçš„
            mask_valid = df_finished_parts['base_material_id'].isin(valid_base_ids)
            df_finished_parts_valid = df_finished_parts[mask_valid].copy()
            df_finished_parts_invalid = df_finished_parts[~mask_valid].copy()
            
            app_logger.info(f"æˆå“æ’¥æ–™åˆ†æµçµæœ: ç¬¦åˆ={len(df_finished_parts_valid)}, ä¸ç¬¦åˆ={len(df_finished_parts_invalid)}")
            
            # --- è™•ç†ä¸»å„€è¡¨æ¿è³‡æ–™ (æ’¥æ–™ + ç¬¦åˆçš„æˆå“æ’¥æ–™ + å‚™æ–™åŠæˆå“) ---
            df_demand = pd.concat([df_wip_parts, df_finished_parts_valid, df_prep_semi_finished], ignore_index=True)
            
            # è¨ˆç®—ç¸½éœ€æ±‚
            df_total_demand = df_demand.groupby('ç‰©æ–™')['æœªçµæ•¸é‡ (EINHEIT)'].sum().reset_index()
            df_total_demand.rename(columns={'æœªçµæ•¸é‡ (EINHEIT)': 'total_demand'}, inplace=True)
            
            # å»ºç«‹éœ€æ±‚è©³æƒ…å°æ‡‰è¡¨
            df_demand['éœ€æ±‚æ—¥æœŸ'] = pd.to_datetime(df_demand['éœ€æ±‚æ—¥æœŸ'], errors='coerce')
            demand_details_map = df_demand.groupby('ç‰©æ–™').apply(
                lambda x: x[['è¨‚å–®', 'æœªçµæ•¸é‡ (EINHEIT)', 'éœ€æ±‚æ—¥æœŸ']].to_dict('records'), include_groups=False
            ).to_dict()
            
            # --- è™•ç†æˆå“å„€è¡¨æ¿è³‡æ–™ (ä¸ç¬¦åˆçš„æˆå“æ’¥æ–™) ---
            df_finished_demand = df_finished_parts_invalid.copy()
            df_total_finished_demand = df_finished_demand.groupby('ç‰©æ–™')['æœªçµæ•¸é‡ (EINHEIT)'].sum().reset_index()
            df_total_finished_demand.rename(columns={'æœªçµæ•¸é‡ (EINHEIT)': 'total_demand'}, inplace=True)
            
            # æˆå“éœ€æ±‚è©³æƒ…
            df_finished_demand['éœ€æ±‚æ—¥æœŸ'] = pd.to_datetime(df_finished_demand['éœ€æ±‚æ—¥æœŸ'], errors='coerce')
            finished_demand_details_map = df_finished_demand.groupby('ç‰©æ–™').apply(
                lambda x: x[['è¨‚å–®', 'æœªçµæ•¸é‡ (EINHEIT)', 'éœ€æ±‚æ—¥æœŸ']].to_dict('records'), include_groups=False
            ).to_dict()

            # --- å…±é€šè™•ç† ---
            df_specs = pd.read_excel(FilePaths.SPECS_FILE)
            df_work_order_summary = DataService._load_work_order_summary()
            df_on_order = DataService._load_on_order_data()
            
            # è™•ç†åœ¨é€”æ•¸é‡
            df_total_on_order = df_on_order.groupby('ç‰©æ–™')['ä»å¾…äº¤è²¨ã€ˆæ•¸é‡ã€‰'].sum().reset_index()
            df_total_on_order.rename(columns={'ä»å¾…äº¤è²¨ã€ˆæ•¸é‡ã€‰': 'on_order_stock'}, inplace=True)
            
            # å»ºç«‹ä¸»è³‡æ–™è¡¨
            df_main = DataService._build_main_dataframe(
                df_total_demand, df_inventory, df_total_on_order, df_demand, material_buyer_map, demand_details_map, part_drawing_map, delivery_schedules_map
            )
            
            # å»ºç«‹æˆå“è³‡æ–™è¡¨
            df_finished_dashboard = DataService._build_main_dataframe(
                df_total_finished_demand, df_inventory, df_total_on_order, df_finished_demand, material_buyer_map, finished_demand_details_map, part_drawing_map, delivery_schedules_map
            )
            
            # å»ºç«‹è¨‚å–®è©³æƒ…å°æ‡‰è¡¨ (åŒ…å«æ‰€æœ‰æˆå“æ’¥æ–™ï¼Œä»¥ä¾¿æŸ¥è©¢)
            order_details_map = DataService._build_order_details_map(
                df_wip_parts, df_finished_parts, df_inventory
            )
            
            # è™•ç†è¦æ ¼è³‡æ–™
            specs_map = DataService._build_specs_map(df_specs)
            
            # æå–å·¥å–®ç¸½è¡¨æ‘˜è¦è³‡è¨Š
            order_summary_map = DataService._build_order_summary_map(df_work_order_summary)
            
            app_logger.info("è³‡æ–™è¼‰å…¥èˆ‡è™•ç†å®Œç•¢ã€‚")
            
            # æ¸…ç† NaN å€¼
            materials_dashboard_cleaned = df_main.fillna('').to_dict(orient='records')
            finished_dashboard_cleaned = df_finished_dashboard.fillna('').to_dict(orient='records')
            specs_data_cleaned = df_specs.fillna('').to_dict(orient='records')
            inventory_data_cleaned = df_inventory.fillna('').to_dict(orient='records')
            demand_details_map_cleaned = replace_nan_in_dict(demand_details_map)
            finished_demand_details_map_cleaned = replace_nan_in_dict(finished_demand_details_map)
            order_details_map_cleaned = replace_nan_in_dict(order_details_map)
            
            # --- è‡ªå‹•åŒæ­¥ç‰©æ–™åˆ°è³‡æ–™åº« ---
            DataService._sync_materials_to_database(df_demand, df_finished_demand, material_buyer_map)
            
            return {
                "materials_dashboard": materials_dashboard_cleaned,
                "finished_dashboard": finished_dashboard_cleaned, # æ–°å¢æˆå“å„€è¡¨æ¿
                "specs_data": specs_data_cleaned,
                "demand_details_map": demand_details_map_cleaned,
                "finished_demand_details_map": finished_demand_details_map_cleaned, # æ–°å¢æˆå“éœ€æ±‚è©³æƒ…
                "order_details_map": order_details_map_cleaned,
                "specs_map": specs_map,
                "order_summary_map": order_summary_map,
                "inventory_data": inventory_data_cleaned  # æ–°å¢å®Œæ•´åº«å­˜è³‡æ–™
            }
        
        except FileNotFoundError as e:
            app_logger.error(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°å¿…è¦çš„è³‡æ–™æª”æ¡ˆã€‚è«‹ç¢ºèªæª”æ¡ˆæ˜¯å¦å­˜åœ¨ä¸”è·¯å¾‘æ­£ç¢ºï¼š{e.filename}", exc_info=True)
            return None
        except Exception as e:
            app_logger.error(f"è™•ç†è³‡æ–™æ™‚ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}", exc_info=True)
            return None
    
    @staticmethod
    def _load_work_order_summary():
        """è¼‰å…¥å·¥å–®ç¸½è¡¨"""
        work_order_summary_path = FilePaths.WORK_ORDER_SUMMARY_FILE
        df_work_order_summary = pd.DataFrame()
        
        if os.path.exists(work_order_summary_path):
            try:
                df_work_order_summary = pd.read_excel(
                    work_order_summary_path, 
                    sheet_name=FilePaths.WORK_ORDER_SUMMARY_SHEET
                )
                # é‡æ–°å‘½åæ¬„ä½ä»¥åŒ¹é…é æœŸ
                if 'å“è™Ÿèªªæ˜' in df_work_order_summary.columns and 'ç‰©æ–™èªªæ˜' not in df_work_order_summary.columns:
                    df_work_order_summary.rename(columns={'å“è™Ÿèªªæ˜': 'ç‰©æ–™èªªæ˜'}, inplace=True)
                app_logger.info(f"DEBUG: df_work_order_summary æ¬„ä½: {df_work_order_summary.columns.tolist()}")
            except Exception as e:
                app_logger.error(f"è¼‰å…¥ '{work_order_summary_path}' çš„ '{FilePaths.WORK_ORDER_SUMMARY_SHEET}' é ç±¤æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        else:
            app_logger.warning(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ° '{work_order_summary_path}' æª”æ¡ˆã€‚å·¥å–®æ‘˜è¦è³‡è¨Šå°‡ç„¡æ³•è¼‰å…¥ã€‚")
        
        return df_work_order_summary
    
    @staticmethod
    def _load_on_order_data():
        """è¼‰å…¥å·²è¨‚æœªäº¤è³‡æ–™ä¸¦åŒæ­¥åˆ°è³‡æ–™åº«"""
        on_order_path = FilePaths.ON_ORDER_FILE
        
        if not os.path.exists(on_order_path):
            app_logger.warning("è­¦å‘Šï¼šæ‰¾ä¸åˆ° 'å·²è¨‚æœªäº¤.XLSX' æª”æ¡ˆã€‚")
            return pd.DataFrame(columns=['ç‰©æ–™', 'ä»å¾…äº¤è²¨ã€ˆæ•¸é‡ã€‰'])
        
        # è®€å– Excel æª”æ¡ˆ
        df_on_order = pd.read_excel(on_order_path)
        app_logger.info(f"å·²è®€å– {len(df_on_order)} ç­†æ¡è³¼å–®è³‡æ–™")
        
        # åŒæ­¥åˆ°è³‡æ–™åº«
        try:
            DataService._sync_purchase_orders_to_db(df_on_order)
            app_logger.info("æ¡è³¼å–®åŒæ­¥å®Œæˆ")
        except Exception as e:
            app_logger.error(f"æ¡è³¼å–®åŒæ­¥å¤±æ•—: {e}", exc_info=True)
        
        return df_on_order
    
    @staticmethod
    def _build_main_dataframe(df_total_demand, df_inventory, df_total_on_order, df_demand, material_buyer_map=None, demand_details_map=None, part_drawing_map=None, delivery_schedules_map=None):
        """å»ºç«‹ä¸»è³‡æ–™è¡¨"""
        # ä»¥ç¸½éœ€æ±‚ç‚ºåŸºç¤ï¼Œç¢ºä¿æ‰€æœ‰æœ‰éœ€æ±‚çš„ç‰©æ–™éƒ½è¢«åŒ…å«
        df_main = df_total_demand.copy()
        
        # åˆä½µåº«å­˜è³‡è¨Š
        df_main = pd.merge(
            df_main, 
            df_inventory[['ç‰©æ–™', 'ç‰©æ–™èªªæ˜', 'å„²å­˜åœ°é»', 'åŸºç¤è¨ˆé‡å–®ä½', 'æœªé™åˆ¶', 'åœ¨é€”å’Œç§»è½‰', 'å“è³ªæª¢é©—ä¸­', 'é™åˆ¶ä½¿ç”¨åº«å­˜', 'é–’ç½®å¤©æ•¸']], 
            on='ç‰©æ–™', 
            how='left'
        )
        
        # åˆä½µåœ¨é€”æ•¸é‡
        df_main = pd.merge(df_main, df_total_on_order, on='ç‰©æ–™', how='left')
        df_main['total_demand'] = df_main['total_demand'].fillna(0)
        df_main['on_order_stock'] = df_main['on_order_stock'].fillna(0)
        
        df_main.rename(columns={'æœªé™åˆ¶': 'unrestricted_stock', 'å“è³ªæª¢é©—ä¸­': 'inspection_stock'}, inplace=True)
        
        numeric_cols = ['unrestricted_stock', 'inspection_stock', 'total_demand', 'on_order_stock']
        for col in numeric_cols:
            df_main[col] = pd.to_numeric(df_main[col], errors='coerce').fillna(0)
        
        # è¨ˆç®—ç¼ºæ–™æƒ…æ³
        df_main['current_shortage'] = df_main['total_demand'] - (df_main['unrestricted_stock'] + df_main['inspection_stock'])
        df_main['projected_shortage'] = df_main['total_demand'] - (df_main['unrestricted_stock'] + df_main['inspection_stock'] + df_main['on_order_stock'])
        df_main['current_shortage'] = df_main['current_shortage'].clip(lower=0)
        df_main['projected_shortage'] = df_main['projected_shortage'].clip(lower=0)
        
        # è¨ˆç®—æœªä¾†30æ—¥å…§æ˜¯å¦æœ‰éœ€æ±‚ç¼ºæ–™
        df_main['shortage_within_30_days'] = DataService._check_shortage_within_days(
            df_main, demand_details_map, delivery_schedules_map, days=30
        )
        
        # ç¢ºä¿ç‰©æ–™èªªæ˜æ¬„ä½ä¸ç‚ºç©º
        df_material_descriptions = df_demand[['ç‰©æ–™', 'ç‰©æ–™èªªæ˜']].drop_duplicates(subset=['ç‰©æ–™'])
        df_main = pd.merge(df_main, df_material_descriptions, on='ç‰©æ–™', how='left', suffixes=('', '_demand'))
        df_main['ç‰©æ–™èªªæ˜'] = df_main['ç‰©æ–™èªªæ˜'].fillna(df_main['ç‰©æ–™èªªæ˜_demand'])
        df_main.drop(columns=['ç‰©æ–™èªªæ˜_demand'], inplace=True)
        
        # æ’é™¤ç‰©æ–™è™Ÿç¢¼ä»¥ '08' é–‹é ­çš„ç‰©æ–™
        df_main = df_main[~df_main['ç‰©æ–™'].astype(str).str.startswith('08')]
        df_main['base_material_id'] = df_main['ç‰©æ–™'].astype(str).str[:10]
        
        # åŠ å…¥æ¡è³¼äººå“¡è³‡è¨Šï¼ˆä½¿ç”¨å‰10ç¢¼å°æ‡‰ï¼‰
        if material_buyer_map:
            df_main['æ¡è³¼äººå“¡'] = df_main['base_material_id'].map(material_buyer_map).fillna('')
        else:
            df_main['æ¡è³¼äººå“¡'] = ''
        
        # åŠ å…¥åœ–è™Ÿè³‡è¨Š
        if part_drawing_map:
            # å˜—è©¦ä½¿ç”¨å®Œæ•´ç‰©æ–™è™Ÿç¢¼åŒ¹é…ï¼Œä¹Ÿå˜—è©¦å‰10ç¢¼åŒ¹é…
            df_main['drawing_number'] = df_main['ç‰©æ–™'].map(part_drawing_map)
            
            # å¦‚æœé‚„æœ‰ç©ºçš„ï¼Œå†å˜—è©¦ç”¨ base_material_id åŒ¹é…
            mask_empty = df_main['drawing_number'].isna()
            df_main.loc[mask_empty, 'drawing_number'] = df_main.loc[mask_empty, 'base_material_id'].map(part_drawing_map)
            
            df_main['drawing_number'] = df_main['drawing_number'].fillna('')
        else:
            df_main['drawing_number'] = ''
        
        return df_main
    
    @staticmethod
    def _build_order_details_map(df_wip_parts, df_finished_parts, df_inventory):
        """å»ºç«‹è¨‚å–®è©³æƒ…å°æ‡‰è¡¨"""
        df_order_materials = pd.concat([df_wip_parts, df_finished_parts], ignore_index=True)
        df_order_materials = df_order_materials[~df_order_materials['ç‰©æ–™'].astype(str).str.startswith('08')]
        df_order_materials = pd.merge(df_order_materials, df_inventory, on='ç‰©æ–™', how='left')
        
        df_order_materials.rename(columns={
            'æœªé™åˆ¶': 'unrestricted_stock',
            'å“è³ªæª¢é©—ä¸­': 'inspection_stock'
        }, inplace=True)
        
        numeric_cols_order = ['æœªçµæ•¸é‡ (EINHEIT)', 'unrestricted_stock', 'inspection_stock']
        for col in numeric_cols_order:
            df_order_materials[col] = pd.to_numeric(df_order_materials[col], errors='coerce').fillna(0)
        
        app_logger.info(f"DEBUG: df_order_materials (after numeric conversion) çš„æ¬„ä½ç‚º: {df_order_materials.columns.tolist()}")
        
        df_order_materials['order_shortage'] = (
            df_order_materials['æœªçµæ•¸é‡ (EINHEIT)'] - 
            (df_order_materials['unrestricted_stock'] + df_order_materials['inspection_stock'])
        ).clip(lower=0)
        
        order_details_map = df_order_materials.groupby('è¨‚å–®').apply(
            lambda x: x[[ 
                'ç‰©æ–™', 'ç‰©æ–™èªªæ˜_x', 'éœ€æ±‚æ•¸é‡ (EINHEIT)', 'é ˜æ–™æ•¸é‡ (EINHEIT)',
                'æœªçµæ•¸é‡ (EINHEIT)', 'éœ€æ±‚æ—¥æœŸ', 'unrestricted_stock',
                'inspection_stock', 'order_shortage'
            ]].rename(columns={'ç‰©æ–™èªªæ˜_x': 'ç‰©æ–™èªªæ˜'}).to_dict('records'), include_groups=False
        ).to_dict()
        
        return order_details_map
    
    @staticmethod
    def _build_specs_map(df_specs):
        """å»ºç«‹è¦æ ¼å°æ‡‰è¡¨"""
        # ç¢ºä¿è¦æ ¼è¡¨ä¸­çš„ã€Œè¨‚å–®ã€æ¬„ä½æ˜¯å­—ä¸²ï¼Œä¸¦åªæå–æ•¸å­—éƒ¨åˆ†ä½œç‚ºè¨‚å–®è™Ÿç¢¼
        df_specs['è¨‚å–®'] = df_specs['è¨‚å–®'].astype(str).str.extract(r'(\d+)')[0]
        
        # ä¿®æ­£ï¼šåœ¨è½‰æ›ç‚ºå­—å…¸å‰ï¼Œå°‡æ‰€æœ‰ NaN å€¼æ›¿æ›ç‚ºç©ºå­—ä¸²
        specs_map = df_specs.fillna('').groupby('è¨‚å–®').apply(
            lambda x: x.to_dict('records'), include_groups=False
        ).to_dict()
        
        return specs_map
    
    @staticmethod
    def _build_order_summary_map(df_work_order_summary):
        """å»ºç«‹å·¥å–®æ‘˜è¦å°æ‡‰è¡¨"""
        order_summary_map = {}
        
        if not df_work_order_summary.empty:
            required_cols = [
                'å·¥å–®è™Ÿç¢¼', 'ä¸‹å–®å®¢æˆ¶åç¨±', 'ç‰©æ–™èªªæ˜', 'ç”Ÿç”¢é–‹å§‹', 'ç”Ÿç”¢çµæŸ',
                'æ©Ÿæ¢°å¤–åŒ…', 'é›»æ§å¤–åŒ…', 'å™´æ¼†å¤–åŒ…', 'éŸèŠ±å¤–åŒ…', 'æ†åŒ…å¤–åŒ…'
            ]
            
            # ç¢ºä¿æ‰€æœ‰éœ€è¦çš„æ¬„ä½éƒ½å­˜åœ¨
            existing_cols = [col for col in required_cols if col in df_work_order_summary.columns]
            if len(existing_cols) != len(required_cols):
                app_logger.warning(f"è­¦å‘Šï¼š'å·¥å–®ç¸½è¡¨2025.xls' ä¸­ç¼ºå°‘éƒ¨åˆ†é æœŸæ¬„ä½ã€‚é æœŸ: {required_cols}, å¯¦éš›: {df_work_order_summary.columns.tolist()}")
            
            # ç¯©é¸å‡ºåŒ…å«æ‰€æœ‰å¿…è¦æ¬„ä½çš„ DataFrame
            df_filtered_summary = df_work_order_summary[existing_cols].copy()
            
            # è™•ç†æ—¥æœŸæ¬„ä½
            for date_col in ['ç”Ÿç”¢é–‹å§‹', 'ç”Ÿç”¢çµæŸ']:
                if date_col in df_filtered_summary.columns:
                    df_filtered_summary[date_col] = pd.to_datetime(df_filtered_summary[date_col], errors='coerce').dt.strftime('%Y-%m-%d').fillna('')
            
            # è™•ç† NaN å€¼
            df_filtered_summary = df_filtered_summary.fillna('')
            
            # è™•ç†é‡è¤‡çš„ 'å·¥å–®è™Ÿç¢¼'
            if 'å·¥å–®è™Ÿç¢¼' in df_filtered_summary.columns:
                df_filtered_summary['å·¥å–®è™Ÿç¢¼'] = df_filtered_summary['å·¥å–®è™Ÿç¢¼'].astype(str)
                df_filtered_summary.drop_duplicates(subset=['å·¥å–®è™Ÿç¢¼'], keep='first', inplace=True)
            
            # æ ¹æ“š 'å·¥å–®è™Ÿç¢¼' å»ºç«‹æ˜ å°„
            order_summary_map = df_filtered_summary.set_index('å·¥å–®è™Ÿç¢¼').to_dict(orient='index')
            app_logger.info(f"DEBUG: order_summary_map ä¸­åŒ…å«çš„å·¥å–®: {list(order_summary_map.keys())[:5]}... (å‰5å€‹)")
        
        return order_summary_map
    
    @staticmethod
    def _sync_materials_to_database(df_demand, df_finished_demand, material_buyer_map):
        '''
        è‡ªå‹•åŒæ­¥ç‰©æ–™åˆ°è³‡æ–™åº«
        å°‡è¨‚å–®éœ€æ±‚ä¸­çš„ç‰©æ–™è‡ªå‹•åŠ å…¥åˆ° materials è³‡æ–™è¡¨
        
        Args:
            df_demand: ä¸»å„€è¡¨æ¿éœ€æ±‚è³‡æ–™
            df_finished_demand: æˆå“å„€è¡¨æ¿éœ€æ±‚è³‡æ–™
            material_buyer_map: ç‰©æ–™æ¡è³¼äººå“¡å°æ‡‰è¡¨
        '''
        try:
            # åˆä½µæ‰€æœ‰éœ€æ±‚ç‰©æ–™
            all_materials_df = pd.concat([df_demand, df_finished_demand], ignore_index=True)
            
            # å»é‡ä¸¦å–å¾—å”¯ä¸€çš„ç‰©æ–™æ¸…å–®
            unique_materials = all_materials_df[['ç‰©æ–™', 'ç‰©æ–™èªªæ˜']].drop_duplicates(subset=['ç‰©æ–™'])
            
            # éæ¿¾æ‰ä»¥ '08' é–‹é ­çš„ç‰©æ–™
            unique_materials = unique_materials[~unique_materials['ç‰©æ–™'].astype(str).str.startswith('08')]
            
            app_logger.info(f'é–‹å§‹åŒæ­¥ç‰©æ–™åˆ°è³‡æ–™åº«ï¼Œå…± {len(unique_materials)} ç­†ç‰©æ–™')
            
            sync_count = 0
            skip_count = 0
            error_count = 0
            
            for _, row in unique_materials.iterrows():
                try:
                    material_id = str(row['ç‰©æ–™'])
                    description = str(row['ç‰©æ–™èªªæ˜']) if pd.notna(row['ç‰©æ–™èªªæ˜']) else ''
                    base_material_id = material_id[:10] if len(material_id) >= 10 else material_id
                    
                    # æª¢æŸ¥å‰10ç¢¼æ˜¯å¦å·²å­˜åœ¨ï¼ˆä»»ä½•ç‰ˆæœ¬ï¼‰
                    existing_material = Material.query.filter_by(base_material_id=base_material_id).first()
                    
                    if existing_material:
                        skip_count += 1
                        continue  # å‰10ç¢¼å·²å­˜åœ¨ï¼Œè·³é
                    
                    # æŸ¥æ‰¾æ¡è³¼äººå“¡
                    buyer = None
                    buyer_value = material_buyer_map.get(base_material_id)
                    if buyer_value:
                        # å…ˆå˜—è©¦ç”¨ full_name æŸ¥æ‰¾
                        buyer = User.query.filter_by(full_name=buyer_value, role='buyer').first()
                        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå†å˜—è©¦ç”¨ id æŸ¥æ‰¾
                        if not buyer:
                            buyer = User.query.filter_by(id=buyer_value, role='buyer').first()
                    
                    # å»ºç«‹æ–°ç‰©æ–™è¨˜éŒ„
                    new_material = Material(
                        material_id=material_id,
                        description=description,
                        base_material_id=base_material_id,
                        buyer_id=buyer.id if buyer else None,
                        created_at=get_taiwan_time(),
                        updated_at=get_taiwan_time()
                    )
                    
                    db.session.add(new_material)
                    sync_count += 1
                    
                    # æ¯ 100 ç­†æäº¤ä¸€æ¬¡ï¼Œé¿å…è¨˜æ†¶é«”å•é¡Œ
                    if sync_count % 100 == 0:
                        db.session.commit()
                        app_logger.info(f'å·²åŒæ­¥ {sync_count} ç­†ç‰©æ–™åˆ°è³‡æ–™åº«')
                
                except Exception as e:
                    error_count += 1
                    app_logger.warning(f'åŒæ­¥ç‰©æ–™ {material_id} å¤±æ•—: {e}')
                    continue
            
            # æœ€å¾Œæäº¤å‰©é¤˜çš„è³‡æ–™
            if sync_count % 100 != 0:
                db.session.commit()
            
            app_logger.info(f'ç‰©æ–™åŒæ­¥å®Œæˆ: æ–°å¢ {sync_count} ç­†, è·³é {skip_count} ç­†å·²å­˜åœ¨ç‰©æ–™, éŒ¯èª¤ {error_count} ç­†')
            
        except Exception as e:
            db.session.rollback()
            app_logger.error(f'åŒæ­¥ç‰©æ–™åˆ°è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}', exc_info=True)
    
    @staticmethod
    def _check_shortage_within_days(df_materials, demand_details_map, delivery_schedules_map=None, days=30):
        '''
        æª¢æŸ¥ç‰©æ–™æ˜¯å¦åœ¨æœªä¾†æŒ‡å®šå¤©æ•¸å…§æœ‰éœ€æ±‚ç¼ºæ–™
        
        Args:
            df_materials: ç‰©æ–™DataFrame
            demand_details_map: éœ€æ±‚è©³æƒ…å°æ‡‰è¡¨
            delivery_schedules_map: äº¤æœŸåˆ†æ‰¹å°æ‡‰è¡¨ (ğŸ†•)
            days: æª¢æŸ¥å¤©æ•¸ï¼ˆé è¨­30å¤©ï¼‰
            
        Returns:
            Series: å¸ƒæ—å€¼åºåˆ—ï¼ŒTrueè¡¨ç¤ºåœ¨æŒ‡å®šå¤©æ•¸å…§æœƒç¼ºæ–™
        '''
        from datetime import datetime, timedelta
        if delivery_schedules_map is None:
            delivery_schedules_map = {}
            
        now = get_taiwan_time()
        cutoff_date = pd.Timestamp(now + timedelta(days=days))
        shortage_flags = []
        
        for _, material in df_materials.iterrows():
            material_id = material['ç‰©æ–™']
            available_stock = float(material.get('unrestricted_stock', 0) + material.get('inspection_stock', 0))
            
            # 1. å–å¾—éœ€æ±‚èˆ‡åˆ°è²¨çš„æ™‚é–“è»¸äº‹ä»¶
            timeline_events = []
            
            # åŠ å…¥éœ€æ±‚äº‹ä»¶
            demand_details = demand_details_map.get(material_id, [])
            for demand in demand_details:
                demand_date = demand.get('éœ€æ±‚æ—¥æœŸ')
                if pd.notna(demand_date) and demand_date <= cutoff_date:
                    timeline_events.append({
                        'date': demand_date,
                        'type': 'demand',
                        'quantity': float(demand.get('æœªçµæ•¸é‡ (EINHEIT)', 0))
                    })
            
            # åŠ å…¥åˆ°è²¨äº‹ä»¶ (ğŸ†• åˆ†æ‰¹äº¤æœŸ)
            schedules = delivery_schedules_map.get(material_id, [])
            for s in schedules:
                delivery_date = pd.Timestamp(s['expected_date'])
                if delivery_date <= cutoff_date:
                    timeline_events.append({
                        'date': delivery_date,
                        'type': 'delivery',
                        'quantity': s['quantity']
                    })
            
            # 2. æŒ‰æ—¥æœŸæ’åº (åˆ°è²¨æ’åœ¨éœ€æ±‚ä¹‹å‰ï¼Œå¦‚æœåŒä¸€å¤©)
            timeline_events.sort(key=lambda x: (x['date'], 0 if x['type'] == 'delivery' else 1))
            
            # 3. æ¨¡æ“¬åº«å­˜æ°´ä½
            running_stock = available_stock
            has_shortage = False
            
            for event in timeline_events:
                if event['type'] == 'demand':
                    running_stock -= event['quantity']
                    if running_stock < 0:
                        has_shortage = True
                        break
                else: # delivery
                    running_stock += event['quantity']
            
            shortage_flags.append(has_shortage)
        
        return pd.Series(shortage_flags, index=df_materials.index)
    
    @staticmethod
    def _sync_purchase_orders_to_db(df_on_order):
        """
        åŒæ­¥æ¡è³¼å–®åˆ°è³‡æ–™åº«
        
        åŠŸèƒ½ï¼š
        1. æ›´æ–°/å»ºç«‹ purchase_orders è¡¨
        2. åŒæ­¥ç‰©æ–™çš„ buyer_idï¼ˆå‰10ç¢¼åŒ¹é…ï¼‰
        3. æ™ºæ…§åˆ¤æ–·å·²åˆªé™¤æ¡è³¼å–®çš„ç‹€æ…‹
        """
        from datetime import timedelta
        
        # å»ºç«‹ Excel ä¸­çš„æ¡è³¼å–®è™Ÿé›†åˆ
        excel_po_numbers = set()
        
        # å»ºç«‹ç‰©æ–™ -> æ¡è³¼ç¾¤çµ„çš„å°æ‡‰è¡¨ï¼ˆç”¨æ–¼åŒæ­¥ buyer_idï¼‰
        material_buyer_map = {}
        
        success_count = 0
        error_count = 0
        
        for index, row in df_on_order.iterrows():
            try:
                # å»ºç«‹å”¯ä¸€çš„æ¡è³¼å–®è™Ÿ
                po_number = f"{row['æ¡è³¼æ–‡ä»¶']}-{row['é …ç›®']}"
                excel_po_numbers.add(po_number)
                
                material_id = str(row['ç‰©æ–™']).strip()
                base_material_id = material_id[:10] if len(material_id) >= 10 else material_id
                
                # è™•ç†æ¡è³¼ç¾¤çµ„ï¼ˆè£œé›¶åˆ°3ä½æ•¸ï¼‰
                purchase_group = DataService._process_purchase_group(row.get('æ¡è³¼ç¾¤çµ„'))
                
                # è¨˜éŒ„ç‰©æ–™çš„æ¡è³¼ç¾¤çµ„ï¼ˆç”¨æ–¼å¾ŒçºŒåŒæ­¥ï¼‰
                if purchase_group:
                    material_buyer_map[material_id] = {
                        'base_material_id': base_material_id,
                        'purchase_group': purchase_group
                    }
                
                # ç¢ºä¿ç‰©æ–™å­˜åœ¨
                DataService._ensure_material_exists(
                    material_id, 
                    base_material_id, 
                    purchase_group,
                    str(row.get('çŸ­æ–‡', ''))
                )
                
                # æ›´æ–°/å»ºç«‹æ¡è³¼å–®
                DataService._update_or_create_purchase_order(row, po_number, material_id, purchase_group)
                
                success_count += 1
                
                # æ¯ 100 ç­†æäº¤ä¸€æ¬¡
                if success_count % 100 == 0:
                    db.session.commit()
                    app_logger.info(f"å·²è™•ç† {success_count} ç­†æ¡è³¼å–®...")
            
            except Exception as e:
                error_count += 1
                app_logger.error(f"è™•ç†æ¡è³¼å–®å¤±æ•—: {e}")
                continue
        
        # æœ€å¾Œæäº¤
        db.session.commit()
        
        # æ™ºæ…§åˆ¤æ–·å·²åˆªé™¤çš„æ¡è³¼å–®
        DataService._handle_deleted_purchase_orders(excel_po_numbers)
        
        # åŒæ­¥ç‰©æ–™çš„ buyer_idï¼ˆå‰10ç¢¼åŒ¹é…ï¼‰
        DataService._sync_materials_buyer_id(material_buyer_map)
        
        app_logger.info(f"æ¡è³¼å–®åŒæ­¥å®Œæˆ: æˆåŠŸ {success_count} ç­†, å¤±æ•— {error_count} ç­†")
    
    @staticmethod
    def _process_purchase_group(pg_value):
        """è™•ç†æ¡è³¼ç¾¤çµ„ï¼Œè£œé›¶åˆ°3ä½æ•¸"""
        if pd.isna(pg_value):
            return None
        
        if isinstance(pg_value, (int, float)):
            return str(int(pg_value)).zfill(3)
        else:
            pg_str = str(pg_value).strip()
            if pg_str.isdigit():
                return pg_str.zfill(3)
            else:
                return pg_str
    
    @staticmethod
    def _ensure_material_exists(material_id, base_material_id, buyer_id, description):
        """ç¢ºä¿ç‰©æ–™å­˜åœ¨ï¼Œä¸å­˜åœ¨å‰‡å»ºç«‹ï¼ˆä»¥å‰10ç¢¼ç‚ºåŸºæº–ï¼‰"""
        # æª¢æŸ¥å‰10ç¢¼æ˜¯å¦å·²å­˜åœ¨ï¼ˆä»»ä½•ç‰ˆæœ¬ï¼‰
        existing = Material.query.filter_by(base_material_id=base_material_id).first()
        
        if not existing:
            # åªåœ¨å‰10ç¢¼ä¸å­˜åœ¨æ™‚æ‰å»ºç«‹æ–°ç‰©æ–™
            material = Material(
                material_id=material_id,
                base_material_id=base_material_id,
                buyer_id=buyer_id,
                description=description if pd.notna(description) else None,
                created_at=get_taiwan_time(),
                updated_at=get_taiwan_time()
            )
            db.session.add(material)
            app_logger.info(f"å»ºç«‹æ–°ç‰©æ–™ï¼ˆå‰10ç¢¼ï¼‰: {base_material_id} (ä½¿ç”¨ç‰ˆæœ¬: {material_id})")
    
    @staticmethod
    def _update_or_create_purchase_order(row, po_number, material_id, purchase_group):
        """æ›´æ–°æˆ–å»ºç«‹æ¡è³¼å–®è¨˜éŒ„"""
        po = PurchaseOrder.query.filter_by(po_number=po_number).first()
        
        # ğŸ†• è¨˜éŒ„ä¹‹å‰çš„ç‹€æ…‹ï¼Œç”¨æ–¼æª¢æ¸¬ç‹€æ…‹è®ŠåŒ–
        old_status = po.status if po else None
        old_received_qty = po.received_quantity if po else 0
        
        if not po:
            po = PurchaseOrder()
            po.po_number = po_number
        
        # æ›´æ–°æ‰€æœ‰æ¬„ä½
        po.material_id = material_id
        po.supplier = str(row['ä¾›æ‡‰å•†/ä¾›æ‡‰å·¥å» ']) if pd.notna(row.get('ä¾›æ‡‰å•†/ä¾›æ‡‰å·¥å» ')) else None
        po.item_number = int(row['é …ç›®']) if pd.notna(row.get('é …ç›®')) else None
        po.description = str(row['çŸ­æ–‡']) if pd.notna(row.get('çŸ­æ–‡')) else None
        
        if pd.notna(row.get('æ–‡ä»¶æ—¥æœŸ')):
            po.document_date = pd.to_datetime(row['æ–‡ä»¶æ—¥æœŸ']).date()
        
        po.document_type = str(row['æ¡è³¼æ–‡ä»¶é¡å‹']) if pd.notna(row.get('æ¡è³¼æ–‡ä»¶é¡å‹')) else None
        po.purchase_group = purchase_group
        po.plant = str(row['å·¥å» ']) if pd.notna(row.get('å·¥å» ')) else None
        
        if pd.notna(row.get('å„²å­˜åœ°é»')):
            po.storage_location = str(int(row['å„²å­˜åœ°é»']))
        
        # æ•¸é‡
        po.ordered_quantity = float(row['æ¡è³¼å–®æ•¸é‡']) if pd.notna(row.get('æ¡è³¼å–®æ•¸é‡')) else 0
        po.outstanding_quantity = float(row['ä»å¾…äº¤è²¨ã€ˆæ•¸é‡ã€‰']) if pd.notna(row.get('ä»å¾…äº¤è²¨ã€ˆæ•¸é‡ã€‰')) else 0
        po.received_quantity = po.ordered_quantity - po.outstanding_quantity
        
        # ç‹€æ…‹è¨ˆç®—
        new_status = None
        if po.outstanding_quantity == 0:
            new_status = 'completed'
        elif po.received_quantity > 0:
            new_status = 'partial'
        else:
            new_status = 'pending'
        
        po.status = new_status
        
        # ğŸ†• æª¢æ¸¬éƒ¨åˆ†äº¤è²¨ï¼šç‹€æ…‹è®Šæˆ partial ä¸”æœ‰æ–°çš„æ”¶è²¨æ•¸é‡
        if new_status == 'partial' and (old_status != 'partial' or po.received_quantity > old_received_qty):
            # æ¨™è¨˜è©²ç‰©æ–™çš„æ‰‹å‹•äº¤æœŸéœ€è¦æ›´æ–°
            DataService._mark_delivery_for_partial_receipt(material_id, po_number, po.received_quantity, po.outstanding_quantity)
        
        if not PurchaseOrder.query.filter_by(po_number=po_number).first():
            db.session.add(po)
    
    @staticmethod
    def _handle_deleted_purchase_orders(excel_po_numbers):
        """
        è™•ç†å·²åˆªé™¤çš„æ¡è³¼å–®ç‹€æ…‹
        
        ç°¡åŒ–é‚è¼¯ï¼šå¾ Excel æ¶ˆå¤± = å·²å®Œæˆäº¤è²¨
        """
        # æŸ¥è©¢è³‡æ–™åº«ä¸­æ‰€æœ‰æœªå®Œæˆçš„æ¡è³¼å–®
        all_db_pos = PurchaseOrder.query.filter(
            PurchaseOrder.status.in_(['pending', 'partial'])
        ).all()
        
        updated_count = 0
        today = get_taiwan_time().date()
        
        for po in all_db_pos:
            # å¦‚æœæ¡è³¼å–®åœ¨ Excel ä¸­ï¼Œè·³éï¼ˆä¸æ˜¯å·²åˆªé™¤ï¼‰
            if po.po_number in excel_po_numbers:
                continue
            
            # å¾ Excel æ¶ˆå¤± = å·²å®Œæˆäº¤è²¨
            po.status = 'completed'
            po.actual_delivery_date = today
            po.received_quantity = po.ordered_quantity
            po.outstanding_quantity = 0
            updated_count += 1
            app_logger.info(f"æ¡è³¼å–® {po.po_number} å·²å¾å·²è¨‚æœªäº¤æ¸…å–®ä¸­ç§»é™¤ï¼Œæ¨™è¨˜ç‚ºå·²å®Œæˆ")
        
        if updated_count > 0:
            db.session.commit()
            app_logger.info(f"å·²æ›´æ–° {updated_count} å€‹æ¡è³¼å–®ç‹€æ…‹ç‚ºå·²å®Œæˆ")
    
    @staticmethod
    def _mark_delivery_for_partial_receipt(material_id, po_number, received_qty, outstanding_qty):
        """
        æ¨™è¨˜éƒ¨åˆ†äº¤è²¨çš„äº¤æœŸéœ€è¦æ›´æ–°
        
        ç•¶æ¡è³¼å–®éƒ¨åˆ†äº¤è²¨æ™‚ï¼š
        1. æª¢æŸ¥æ˜¯å¦æœ‰æ‰‹å‹•ç¶­è­·çš„äº¤æœŸ
        2. å¦‚æœäº¤æœŸæœªåˆ°æˆ–å‰›å¥½åˆ°ï¼Œæ¨™è¨˜ç‚ºéœ€è¦ç¢ºèª
        3. åŠ ä¸Šè¨»è¨˜èªªæ˜å·²éƒ¨åˆ†åˆ°è²¨
        """
        try:
            import os
            import json
            from datetime import datetime, timedelta
            
            delivery_file = 'instance/delivery_schedules.json'
            
            if not os.path.exists(delivery_file):
                return
            
            with open(delivery_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            schedules = data.get('delivery_schedules', {}).get(material_id, [])
            
            if not schedules:
                return
            
            # æª¢æŸ¥æ˜¯å¦æœ‰é—œè¯åˆ°é€™å€‹æ¡è³¼å–®çš„äº¤æœŸ
            today = get_taiwan_time().date()
            updated = False
            
            for schedule in schedules:
                # å¦‚æœäº¤æœŸé—œè¯åˆ°é€™å€‹æ¡è³¼å–®
                if schedule.get('po_number') == po_number:
                    try:
                        delivery_date = datetime.fromisoformat(schedule['expected_date']).date()
                        
                        # å¦‚æœäº¤æœŸæœªåˆ°æˆ–å‰›å¥½åˆ°ï¼ˆä»Šå¤©æˆ–æœªä¾†ï¼‰
                        if delivery_date >= today:
                            # ğŸ†• æ¨™è¨˜ç‚ºéƒ¨åˆ†åˆ°è²¨
                            schedule['status'] = 'partial_received'
                            schedule['partial_note'] = f"å·²éƒ¨åˆ†åˆ°è²¨ {received_qty} ä»¶ï¼Œå‰©é¤˜ {outstanding_qty} ä»¶å¾…äº¤"
                            schedule['partial_date'] = get_taiwan_time().isoformat()
                            schedule['needs_update'] = True
                            updated = True
                            app_logger.info(f"ç‰©æ–™ {material_id} çš„æ¡è³¼å–® {po_number} å·²éƒ¨åˆ†åˆ°è²¨ï¼Œäº¤æœŸæ¨™è¨˜ç‚ºéœ€æ›´æ–°")
                    except (ValueError, KeyError):
                        continue
            
            # å„²å­˜æ›´æ–°
            if updated:
                with open(delivery_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                    
        except Exception as e:
            app_logger.error(f"æ¨™è¨˜éƒ¨åˆ†äº¤è²¨äº¤æœŸå¤±æ•—: {e}", exc_info=True)
    
    @staticmethod
    def _sync_materials_buyer_id(material_buyer_map):
        """
        åŒæ­¥ç‰©æ–™çš„ buyer_idï¼ˆå‰10ç¢¼åŒ¹é…ï¼‰
        
        ç­–ç•¥ï¼š
        ä»¥ Excel ç‚ºæº–ï¼Œæ›´æ–°æ‰€æœ‰ç›¸åŒå‰10ç¢¼çš„ç‰©æ–™çš„ buyer_idï¼ˆç„¡è«–åŸæœ¬æ˜¯å¦æœ‰å€¼ï¼‰
        """
        updated_count = 0
        
        for material_id, info in material_buyer_map.items():
            base_material_id = info['base_material_id']
            purchase_group = info['purchase_group']
            
            # æ›´æ–°æ‰€æœ‰ç›¸åŒå‰10ç¢¼çš„ç‰©æ–™ï¼ˆä»¥ Excel ç‚ºæº–ï¼‰
            related_materials = Material.query.filter(
                Material.base_material_id == base_material_id
            ).all()
            
            for material in related_materials:
                material.buyer_id = purchase_group
                material.updated_at = get_taiwan_time()
                updated_count += 1
        
        if updated_count > 0:
            db.session.commit()
            app_logger.info(f"å·²æ›´æ–° {updated_count} å€‹ç‰©æ–™çš„æ¡è³¼äººå“¡è³‡è¨Šï¼ˆå‰10ç¢¼åŒ¹é…ï¼‰")
